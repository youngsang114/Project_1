{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# granger 인과성 검정\n",
    "    - Granger 인과검정의 영가설은 X가 Y에 영향을 미치지 않는다 이다. 즉 H0를 기각해야한다\n",
    "    - 두 개의 시계열 데이터에서 한 변수(한 시계열)의 과거데이터와 다른 한 변수(시계열)의 과거데이터의 결합으로 그 변수(처음 시계열)를 선형 예측(linear regression)을 했을 때 다른 한 변수의 과거데이터로만 선형예측 한 것이 통계적으로 유의미하고 예측에 도움을 줬다면 그것을 그래인저 인과(Granger Causality)가 있다고 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "# df=[]\n",
    "\n",
    "# df.replace(',', '', regex=True, inplace=True) \n",
    "# df = df.apply(pd.to_numeric, errors='coerce') # numeric 변환\n",
    "maxlag= 14\n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "   \n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "# grangers_causation_matrix(df, variables = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "# ax = sns.heatmap(df, annot=True, cmap='Blues')\n",
    "# ax.xaxis.tick_top()\n",
    "# plt.xticks(fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cointegration Test\n",
    "    - significant_yn이 모두 True이면서 dist가 가장 큰 그룹을 찾아보자.\n",
    "    - 공적분 검정(혹은 요한슨 검정)은 다중 시계열 간 적분상 균형관계가 존재하는가를 판단한다\n",
    "    - 상관관계는 시계열 적으로 연관성이 있는지 좀 더 정확한 관계 분석을 위해서 공적분 분석을 진행해보겠습니다.\n",
    "    - 공적분 분석은 statsmodels 라이브러리를 이용해서 coint를 이용\n",
    "    -  반환 값 중 p_value가 존재하는데요. 여기서의 귀무가설은 위에서도 언급했지만 서로 공적분 관계가 없다는 것이 귀무가설입니다\n",
    "    - 따라서 p-value 값이 5%보다 작을 때는 이 귀무가설을 기각하여 서로 공적분 관계가 있다는 것을 알 수 있을겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def calc_sig_dist(cols):\n",
    "    out = coint_johansen(df[cols], 1, 1)\n",
    "    stats = [round(x,2) for x in out.lr1]\n",
    "    sigs = [round(x,2) for x in out.cvt[:, 1]]\n",
    "    yns = [x>y for x,y in zip(stats,sigs)]\n",
    "    dist = np.mean(np.array(stats) - np.array(sigs))\n",
    "    \n",
    "\n",
    "    print('stats: ',stats)                                            ## stats는 공적분 통계량\n",
    "    print('sig-level: ',sigs)                                         ## sig-level은 유의수준 0.05에 해당하는 관측치\n",
    "    print('significant_yn: ',[x>y for x,y in zip(stats,sigs)])        ## significant_yn은 유의 여부(장기 안정성), dist는 안정성의 강도를 의미한다.\n",
    "    print('dist: ', round(dist,2))                                    ## dist는 안정성의 강도를 의미한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Datasets (Train / Test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stationary Test\n",
    "    - 영가설은 정상성을 띄지   --> H0를 기각해야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# adf_sample = adfuller(df_train['samsung_ac'], autolag='AIC') # AIC가 가장 낮은 lag(시차)를 자동 선택\n",
    "# adf_sample\n",
    "\n",
    "# adf_df = pd.DataFrame(adf_sample[:4])\n",
    "# adf_df.columns = ['samsung_ac']\n",
    "# adf_df.index = ['stat','p_value','lag','observ']\n",
    "\n",
    "# sig = pd.DataFrame(data={'samsung_ac':adf_sample[4]['5%']}, index=['5%'])\n",
    "# adf_df = pd.concat([adf_df, sig], axis=0)\n",
    "# adf_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR\n",
    "    - AIC가 10 이상만 되어도 유의한 예측이라고 보기 어렵다.\n",
    "    - 데이터 범위가 커서 예측 수준이 떨어질 수 있다\n",
    "    - 일반적으로 AIC 기준 2.0 이하는 모델이 타당하다고 할 수 있다.\n",
    "    -  전체 평균이 아니라 첫날 데이터를 0으로 기준을 잡고, 그 이후에는 첫날 대비 변화량을 보려줄 수 있도록 할 것이다.\n",
    "    - 이때 다시한번 정상성 여부도 체크해준다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class statsmodels.tsa.vector_ar.var_model.VAR(endog, exog=None, dates=None, freq=None, missing='none')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "VAR 클래스는 벡터 자기회귀(Vector Autoregression) 모델을 구현하는 statsmodels 라이브러리의 API입니다. VAR 클래스의 파라미터(attribute)에 대한 설명은 다음과 같습니다:\n",
    "\n",
    "endog (ndarray): 모델의 종속 변수 데이터로 구성된 2차원 배열입니다. 각 열은 각각의 종속 변수를 나타내며, 행은 시간 인덱스를 나타냅니다.\n",
    "\n",
    "exog (ndarray, optional): 모델의 독립 변수 데이터로 구성된 2차원 배열입니다. 각 열은 각각의 독립 변수를 나타내며, 행은 시간 인덱스를 나타냅니다. 기본값은 None입니다.\n",
    "\n",
    "dates (array_like, optional): 날짜 또는 시간 인덱스를 나타내는 배열입니다. 기본값은 None이며, 인덱스가 없는 경우 None으로 설정됩니다.\n",
    "\n",
    "freq (str, optional): 시계열의 빈도를 나타내는 문자열입니다. 'B', 'D', 'W', 'M', 'A', 등과 같은 주기를 지정할 수 있습니다. 기본값은 None입니다.\n",
    "\n",
    "missing (str, optional): 결측치 처리 방법을 나타내는 문자열입니다. 'none', 'drop', 'raise' 중 하나를 선택할 수 있습니다. 기본값은 'none'으로 결측치를 처리하지 않음을 의미합니다.\n",
    "\n",
    "p (int): 모델의 자기회귀 차수를 나타내는 정수입니다.\n",
    "\n",
    "trend (str, optional): 모델의 추세 항 설정을 나타내는 문자열입니다. 'c' (상수), 'ct' (상수 및 추세), 'ctt' (상수, 추세, 추세의 제곱) 중 하나를 선택할 수 있습니다. 기본값은 'c'입니다.\n",
    "\n",
    "method (str, optional): 모델 추정에 사용할 방법을 나타내는 문자열입니다. 'ols', 'yw' (Yule-Walker), 'burg' (Burg's method), 'ols-rew' (OLS with rewighting), 'burg-rew' (Burg's method with rewighting) 중 하나를 선택할 수 있습니다. 기본값은 'ols'입니다.\n",
    "\n",
    "maxlags (int, optional): 모델 추정에 사용할 최대 자기회귀 차수를 제한하는 정수입니다. 기본값은 None으로, 자동으로 선택됩니다.\n",
    "\n",
    "ic (str, optional): 최적의 모델 차수를 선택하는 정보 기준을 나타내는 문자열입니다. 'aic', 'bic', 'fpe', 'hqic' 중 하나를 선택할 수 있습니다. 기본값은 None으로, 정보 기준을 사용하지 않습니다.\n",
    "\n",
    "trend_offset (int, optional): 추세의 시작 오프셋을 나타내는 정수입니다. 기본값은 1입니다.\n",
    "\n",
    "seasonal (bool, optional): 계절성 구성 요소가 있는 경우 True로 설정하고, 없는 경우 False로 설정합니다. 기본값은 False입니다.\n",
    "\n",
    "periods (int, optional): 계절성 주기의 길이를 나타내는 정수입니다. 기본값은 None으로, 계절성이 없는 경우 None으로 설정됩니다.\n",
    "\n",
    "fir (int, optional): 계절성 주기의 FIR(유한 임펄스 응답) 길이를 나타내는 정수입니다. 기본값은 None으로, 계절성이 없는 경우 None으로 설정됩니다.\n",
    "\n",
    "innov (None or ndarray, optional): 모델의 혁신(innovation) 항을 나타내는 배열입니다. 기본값은 None으로, 모델 학습을 통해 자동으로 추정됩니다.\n",
    "\n",
    "위와 같이 VAR 클래스의 파라미터를 설정하고 모델을 초기화한 후에는 fit(), forecast(), plot_forecast() 등의 메서드를 사용하여 모델을 학습하고 예측 결과를 생성하고 시각화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "# var = VAR(df_train_)\n",
    "#ㄴ\n",
    "\n",
    "# model = var_norm.fit(14)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.resid.corr()로 colleration matrix확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# durbin_watson\n",
    "### 잔차의 독립성 검정은 Durbin Watson 검정을 통해 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# durbin_res = pd.DataFrame([model.resid.columns, \n",
    "#                            [round(x,2) for x in durbin_watson(model.resid)]]).T\n",
    "# durbin_res.set_index([0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범위 : 0~4\n",
    "- 양의 상관관계 : 0에 수렴\n",
    "- 음의 상관관계 : 4에 수렴\n",
    "- 상관관계 없음 : 2에 수렴"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.k_ar\n",
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ins = df_train_norm.values[-model.k_ar:]\n",
    "# ins\n",
    "# 3개 변수에 대한 최근 14일치의 데이터다. 앞에서 우리는 최근 30일치를 기준으로 train dataset를 나눴기 때문에 30일치보다 앞선 14일치 데이터라고 보면 되겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = model.forecast(y=ins, steps=7)\n",
    "# df_f = pd.DataFrame(f, columns=df_train_norm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
